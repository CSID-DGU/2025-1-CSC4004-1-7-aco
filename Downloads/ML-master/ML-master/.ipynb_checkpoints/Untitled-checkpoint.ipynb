{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ec67a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a534a076",
   "metadata": {},
   "source": [
    "### Tic Tac Toe 환경 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c3b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self):\n",
    "    # 보드는 0으로 초기화된 9개의 배열로 준비\n",
    "    # 게임종료 : done = True\n",
    "        self.board_a = np.zeros(16)\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.winner = 0\n",
    "        self.print = False\n",
    "\n",
    "    def move(self, p1, p2, player):\n",
    "    # 각 플레이어가 선택한 행동을 표시 하고 게임 상태(진행 또는 종료)를 판단\n",
    "    # p1 = 1, p2 = -1로 정의\n",
    "    # 각 플레이어는 행동을 선택하는 select_action 메서드를### Tic Tac Toe 환경 정의 가짐\n",
    "        if player == 1:\n",
    "            pos = p1.select_action(env, player)\n",
    "        else:\n",
    "            pos = p2.select_action(env, player)\n",
    "        \n",
    "        # 보드에 플레이어의 선택을 표시\n",
    "        self.board_a[pos] = player\n",
    "        if self.print:\n",
    "            print(player)\n",
    "            self.print_board()\n",
    "        # 게임이 종료상태인지 아닌지를 판단\n",
    "        self.end_check(player)\n",
    "        \n",
    "        return  self.reward, self.done\n",
    " \n",
    "    # 현재 보드 상태에서 가능한 행동(둘 수 있는 장소)을 탐색하고 리스트로 반환\n",
    "    def get_action(self):\n",
    "        observation = []\n",
    "        for i in range(16):\n",
    "            if self.board_a[i] == 0:\n",
    "                observation.append(i)\n",
    "        return observation\n",
    "    \n",
    "    # 게임이 종료(승패 또는 비김)됐는지 판단\n",
    "    def end_check(self,player):\n",
    "        # 0 1 2\n",
    "        # 3 4 5\n",
    "        # 6 7 8\n",
    "        # 승패 조건은 가로, 세로, 대각선 이 -1 이나 1 로 동일할 때 \n",
    "        end_condition = ((0,1,2,3),(4,5,6,7),(8,9,10,11),(12,13,14,15),(0,5,10,15),(3,6,9,12),(0,4,8,12),(1,5,9,13),(2,6,10,14),(3,7,11,15))\n",
    "        for line in end_condition:\n",
    "            if self.board_a[line[0]] == self.board_a[line[1]] \\\n",
    "                and self.board_a[line[1]] == self.board_a[line[2]] \\\n",
    "                    and self.board_a[line[2]] == self.board_a[line[3]] \\\n",
    "                    and self.board_a[line[0]] != 0:\n",
    "                    # 종료됐다면 누가 이겼는지 표시\n",
    "                    self.done = True\n",
    "                    self.reward = player\n",
    "                    return\n",
    "        # 비긴 상태는 더는 보드에 빈 공간이 없을때\n",
    "        observation = self.get_action()\n",
    "        if (len(observation)) == 0:\n",
    "            self.done = True\n",
    "            self.reward = 0            \n",
    "        return\n",
    "        \n",
    "    # 현재 보드의 상태를 표시 p1 = O, p2 = X    \n",
    "    def print_board(self):\n",
    "        print(\"+----+----+----+----+\")\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if self.board_a[4*i+j] == 1:\n",
    "                    print(\"|  O\",end=\" \")\n",
    "                elif self.board_a[4*i+j] == -1:\n",
    "                    print(\"|  X\",end=\" \")\n",
    "                else:\n",
    "                    print(\"|   \",end=\" \")\n",
    "            print(\"|\")\n",
    "            print(\"+----+----+----+----+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063eaa91",
   "metadata": {},
   "source": [
    "### Human player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96bce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Human player\"\n",
    "        \n",
    "    def select_action(self, env, player):\n",
    "        while True:\n",
    "            # 가능한 행동을 조사한 후 표시\n",
    "            available_action = env.get_action()\n",
    "            print(\"possible actions = {}\".format(available_action))\n",
    "\n",
    "            # 상태 번호 표시\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  0 +  1 +  2 +  3 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  4 +  5 +  6 +  7 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  8 +  9 + 10 + 11 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+ 12 + 13 + 14 + 15 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "                        \n",
    "            # 키보드로 가능한 행동을 입력 받음\n",
    "            action = input(\"Select action(human) : \")\n",
    "            action = int(action)\n",
    "            \n",
    "            # 입력받은 행동이 가능한 행동이면 반복문을 탈출\n",
    "            if action in available_action:\n",
    "                return action\n",
    "            # 아니면 행동 입력을 반복\n",
    "            else:\n",
    "                print(\"You selected wrong action\")\n",
    "        return### Human player"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9988c0",
   "metadata": {},
   "source": [
    "### 랜덤 플레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce60b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Random player\"\n",
    "        self.print = False\n",
    "        \n",
    "    def select_action(self, env, player):\n",
    "        # 가능한 행동 조사\n",
    "        available_action = env.get_action()\n",
    "        # 가능한 행동 중 하나를 무작위로 선택\n",
    "        action = np.random.randint(len(available_action))\n",
    "#         print(\"Select action(random) = {}\".format(available_action[action]))\n",
    "        return available_action[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c4972",
   "metadata": {},
   "source": [
    "### 몬테카를로 플레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cdd4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monte_Carlo_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"MC player\"\n",
    "        self.num_playout = 1000\n",
    "        \n",
    "    def select_action(self, env, player):\n",
    "        # 가능한 행동 조사\n",
    "        available_action = env.get_action()\n",
    "        V = np.zeros(len(available_action))\n",
    "        \n",
    "        for i in range(len(available_action)):\n",
    "            # 플레이아웃을 100번 반복\n",
    "            for j in range(self.num_playout):\n",
    "                # 지금 상태를 복사해서 플레이 아웃에 사용\n",
    "                temp_env = copy.deepcopy(env)\n",
    "                # 플레이아웃의 결과는 승리 플레이어의 값으로 반환\n",
    "                # p1 이 이기면 1, p2 가 이기면 -1\n",
    "                self.playout(temp_env, available_action[i], player)\n",
    "                if player == temp_env.reward:\n",
    "                    V[i] += 1\n",
    "   \n",
    "        return available_action[np.argmax(V)]    \n",
    "\n",
    "    # 플레이아웃 재귀함수\n",
    "    # 게임이 종료상태 (승 또는 패 또는 비김) 가 될때까지 행동을 임의로 선택하는 것을 반복\n",
    "    # 플레이어는 계속 바뀌기 때문에 (-)를 곱해서 -1, 1, -1 이 되게함    \n",
    "    def playout(self, temp_env, action, player):\n",
    "        \n",
    "        temp_env.board_a[action] = player\n",
    "        temp_env.end_check(player)\n",
    "        # 게임 종료 체크\n",
    "        if temp_env.done == True:\n",
    "            return \n",
    "        else:\n",
    "            # 플레이어 교체\n",
    "            player = -player\n",
    "            # 가능한 행동 조사\n",
    "            available_action = temp_env.get_action()\n",
    "            # 무작위로 행동을 선택\n",
    "            action = np.random.randint(len(available_action))\n",
    "            self.playout(temp_env, available_action[action], player)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f1961",
   "metadata": {},
   "source": [
    "### 액터크리틱 플레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80a69260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticPlayer:\n",
    "    def __init__(self, critic_lr=0.1, actor_lr=0.1, gamma=0.9, temperature=1.0):\n",
    "        # 식별용 이름\n",
    "        self.name = \"AC_player\"\n",
    "        # Critic 학습률 α, Actor 학습률 β\n",
    "        self.critic_lr = critic_lr  # α for V(s)\n",
    "        self.actor_lr  = actor_lr   # β for h(s,a)\n",
    "        # 감가율 γ\n",
    "        self.gamma     = gamma\n",
    "        # 소프트맥스 탐색 온도 파라미터\n",
    "        self.temperature = temperature\n",
    "        # 상태가치 V(s) 테이블 및 정책 선호도 h(s,a) 테이블\n",
    "        self.V = {}\n",
    "        self.h = {}\n",
    "\n",
    "    def _get_state(self, env):\n",
    "        # 보드를 튜플로 변환하여 해시 가능\n",
    "        return tuple(env.board_a)\n",
    "\n",
    "    def policy(self, state, actions):\n",
    "        # 상태-행동 선호도 배열 생성\n",
    "        prefs = np.array([self.h.get((state, a), 0.0) for a in actions])\n",
    "        # 수치 안정성 위해 최대값 제거\n",
    "        m = prefs.max()\n",
    "        # softmax 계산 (클리핑 포함)\n",
    "        z = np.clip((prefs - m) / self.temperature, -50, 50)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / exp_z.sum()\n",
    "\n",
    "    def select_action(self, env, player=None):\n",
    "        \"\"\"env와 player 인자를 받지만 player는 학습에 사용하지 않음\"\"\"\n",
    "        state = self._get_state(env)\n",
    "        actions = env.get_action()\n",
    "        probs = self.policy(state, actions)\n",
    "        idx = np.random.choice(len(actions), p=probs)\n",
    "        return actions[idx]\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # Critic: TD 오차 계산\n",
    "        v = self.V.get(state, 0.0)\n",
    "        v_next = 0.0 if done else self.V.get(next_state, 0.0)\n",
    "        delta = reward + self.gamma * v_next - v\n",
    "        # Critic 업데이트: V(s) ← V(s) + α·δ\n",
    "        self.V[state] = v + self.critic_lr * delta\n",
    "        # Actor 업데이트: h(s,a) ← h(s,a) + β·δ\n",
    "        key = (state, action)\n",
    "        self.h[key] = self.h.get(key, 0.0) + self.actor_lr * delta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed1061",
   "metadata": {},
   "source": [
    "### 액터크리틱 플레이어 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906bee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 100000/100000 [00:49<00:00, 2023.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 = 31471 p2 = 26890 draw = 41639\n",
      "end train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p1 = ActorCriticPlayer()\n",
    "p2 = ActorCriticPlayer()\n",
    "\n",
    "# 결과 기록용\n",
    "p1_score = 0\n",
    "p2_score = 0\n",
    "draw_score = 0\n",
    "\n",
    "max_games = 100000\n",
    "max_steps_per_game = 16   # 4x4 보드니까 이보다 크면 무한루프 방지\n",
    "\n",
    "for episode in tqdm(range(1, max_games+1)):\n",
    "    env = Environment()\n",
    "    state = tuple(env.board_a)   # 초기 상태\n",
    "    done = False\n",
    "    current, other = p1, p2      # 첫 수는 p1\n",
    "\n",
    "    for step in range(max_steps_per_game):\n",
    "        # 1) 행동 선택\n",
    "        action = current.select_action(env)\n",
    "\n",
    "        # 2) 한 수 두기\n",
    "        env.board_a[action] = 1 if current is p1 else -1\n",
    "        env.end_check(1 if current is p1 else -1)\n",
    "\n",
    "        # 3) 다음 상태, 보상, 종료 여부\n",
    "        next_state = tuple(env.board_a)\n",
    "        reward = env.reward\n",
    "        done = env.done\n",
    "\n",
    "        # 4) 학습\n",
    "        current.learn(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            # 결과 집계\n",
    "            if   reward ==  1:\n",
    "                p1_score += 1\n",
    "            elif reward == -1:\n",
    "                p2_score += 1\n",
    "            else:  # reward == 0\n",
    "                draw_score += 1\n",
    "            break\n",
    "\n",
    "        # 5) 플레이어 교대\n",
    "        current, other = other, current\n",
    "\n",
    "#     # (선택) 탐색 강도 줄이기\n",
    "#     p1.temperature *= 0.9\n",
    "#     p2.temperature *= 0.9\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(\"p1 = {} p2 = {} draw = {}\".format(p1_score,p2_score,draw_score))\n",
    "print(\"end train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa980d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl player : Random player\n",
      "p2 player : AC_player\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |  X |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |  X |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |  X |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |  O |  X |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  X |  O |  X |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  X |  O |  X |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  X |  O |  X |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|  O |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  X |  O |  X |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|  O |  O |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  X |  O |  X |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|  O |  O |    |  O |\n",
      "+----+----+----+----+\n",
      "|    |  X |  O |  X |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|  O |  O |    |  O |\n",
      "+----+----+----+----+\n",
      "|    |  X |  O |  X |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|  O |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "|    |  X |  O |  X |\n",
      "+----+----+----+----+\n",
      "winner is p1(Random player)\n",
      "final result\n",
      "+----+----+----+----+\n",
      "|  X |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  X |    |\n",
      "+----+----+----+----+\n",
      "|  O |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "|    |  X |  O |  X |\n",
      "+----+----+----+----+\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "#p1= Human_player()\n",
    "# p2 = Human_player()\n",
    "\n",
    "p1 = Random_player()\n",
    "# p2 = Random_player()\n",
    "\n",
    "# p1 = Monte_Carlo_player()\n",
    "# p1.num_playout = 100\n",
    "# p2 = Monte_Carlo_player()\n",
    "# p2.num_playout = 1000\n",
    "\n",
    "#p1 = p1_Qplayer\n",
    "#3p1.epsilon = 0\n",
    "\n",
    "#p2 = p2_Qplayer\n",
    "#p2.epsilon = 0\n",
    "\n",
    "# p1 = p1_DQN\n",
    "# p1.epsilon = 0\n",
    "\n",
    "#p1 = AC_Player\n",
    "p2 = ActorCriticPlayer()\n",
    "# 지정된 게임 수를 자동으로 두게 할 것인지 한게임씩 두게 할 것인지 결정\n",
    "# auto = True : 지정된 판수(games)를 자동으로 진행 \n",
    "# auto = False : 한판씩 진행\n",
    "\n",
    "auto = False\n",
    "\n",
    "# auto 모드의 게임수\n",
    "games = 100\n",
    "\n",
    "print(\"pl player : {}\".format(p1.name))\n",
    "print(\"p2 player : {}\".format(p2.name))\n",
    "\n",
    "# 각 플레이어의 승리 횟수를 저장\n",
    "p1_score = 0\n",
    "p2_score = 0\n",
    "draw_score = 0\n",
    "\n",
    "\n",
    "if auto: \n",
    "    # 자동 모드 실행\n",
    "    for j in tqdm(range(games)):\n",
    "        \n",
    "        np.random.seed(j)\n",
    "        env = Environment()\n",
    "        \n",
    "        for i in range(10000):\n",
    "            # p1 과 p2가 번갈아 가면서 게임을 진행\n",
    "            # p1(1) -> p2(-1) -> p1(1) -> p2(-1) ...\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            # 게임 종료 체크\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    draw_score += 1\n",
    "                break\n",
    "\n",
    "else:                \n",
    "    # 한 게임씩 진행하는 수동 모드\n",
    "    np.random.seed(1)\n",
    "    while True:\n",
    "        \n",
    "        env = Environment()\n",
    "        env.print = False\n",
    "        for i in range(10000):\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            env.print_board()\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    print(\"winner is p1({})\".format(p1.name))\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    print(\"winner is p2({})\".format(p2.name))\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    print(\"draw\")\n",
    "                    draw_score += 1\n",
    "                break\n",
    "        \n",
    "        # 최종 결과 출력        \n",
    "        print(\"final result\")\n",
    "        env.print_board()\n",
    "\n",
    "        # 한게임 더?최종 결과 출력 \n",
    "        answer = input(\"More Game? (y/n)\")\n",
    "\n",
    "        if answer == 'n':\n",
    "            break           \n",
    "\n",
    "print(\"p1({}) = {} p2({}) = {} draw = {}\".format(p1.name, p1_score,p2.name, p2_score,draw_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28209621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
